{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "model_name_or_path = 'checkpoints/tinyllama_caption2instruct_ft'\n",
    "model = transformers.LlamaForCausalLM.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        attn_implementation='flash_attention_2',\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    ).cuda()\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from instruct_tri2tri.tinyllama_ft.conversation import conv_llama_2\n",
    "conv = conv_llama_2.copy()\n",
    "roles = conv_llama_2.roles\n",
    "inp = '3D rendering of a white sofa with wooden legs and frame.'\n",
    "conv.append_message(conv_llama_2.roles[0], inp)\n",
    "conv.append_message(conv_llama_2.roles[1], None)\n",
    "prompt = conv.get_prompt()\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "for key, value in inputs.items():\n",
    "    inputs[key] = value.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "j = json.load(open('data/objaverse/cap3d_automated_objaverse_highquality_550k.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '53d0b31aa7f84bc4b1733224963d0114',\n",
       " 'image': 'objaverse/Cap3D_imgs_view0/53d0b31aa7f84bc4b1733224963d0114',\n",
       " 'conversations': [{'from': 'human',\n",
       "   'value': '<triplane>\\nDescribe the 3D model concisely.'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '3D rendering of a white sofa with wooden legs and frame.'}]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(**inputs)\n",
    "# tokenizer.decode(output[0], special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'make the sofa a brown leather sofa'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output[0, inputs.input_ids.shape[1]:], skip_special_tokens=True).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'image': 'objaverse/Cap3D_imgs_view0/53d0b31aa7f84bc4b1733224963d0114',\n",
       "  'caption': '3D rendering of a white sofa with wooden legs and frame.',\n",
       "  'instruct': 'make the sofa a leather sofa'},\n",
       " {'image': 'objaverse/Cap3D_imgs_view0/25f25e35aada40e49194657fd51c1199',\n",
       "  'caption': '3D model of a wooden fence with posts and gate, featuring a sand base.',\n",
       "  'instruct': 'make the fence made of metal'},\n",
       " {'image': 'objaverse/Cap3D_imgs_view0/9ff331513cea44a2938d09a03d7b0493',\n",
       "  'caption': 'White plastic cylinder resembling a toilet paper holder.',\n",
       "  'instruct': 'make it a toilet paper holder made of gold'},\n",
       " {'image': 'objaverse/Cap3D_imgs_view0/f78e38ccea1d45dcaefe1970e7df4035',\n",
       "  'caption': '3D model of a forklift truck with a trailer.',\n",
       "  'instruct': 'make the truck a tank'},\n",
       " {'image': 'objaverse/Cap3D_imgs_view0/112c059282cf4511a01fd27211edcae8',\n",
       "  'caption': 'A 3D model of a wooden table and bench.',\n",
       "  'instruct': 'make it a wooden table and bench made of marble'},\n",
       " {'image': 'objaverse/Cap3D_imgs_view0/77031e12fc7f49c09530addc5c7c1cd3',\n",
       "  'caption': 'A black and white Lego ninja with a bird on his shoulder, holding a sword.',\n",
       "  'instruct': 'make the bird a parrot'},\n",
       " {'image': 'objaverse/Cap3D_imgs_view0/d06cca5175434a83b3944bcfad47c464',\n",
       "  'caption': 'A 3D rendering of a small table with stairs, glass top, and shelf in a room.',\n",
       "  'instruct': 'make the table a coffee table'},\n",
       " {'image': 'objaverse/Cap3D_imgs_view0/037306d9dd4e4a3cbe14d127cfe26fb5',\n",
       "  'caption': 'A 3D model of a small white building with tables, chairs, and a light inside, featuring a lid and beige accents.',\n",
       "  'instruct': 'make the building a castle'},\n",
       " {'image': 'objaverse/Cap3D_imgs_view0/995f110c17cc49b5be4c90ea5028fb1d',\n",
       "  'caption': 'A 3D white paper airplane model',\n",
       "  'instruct': 'make it a helicopter'},\n",
       " {'image': 'objaverse/Cap3D_imgs_view0/24a550a5d6e54402809a75eb8654230e',\n",
       "  'caption': 'A 3D model of a small brick building with a blue roof and green and white walls.',\n",
       "  'instruct': 'make it a castle'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json \n",
    "json.load(open('data/objaverse/cap3d_automated_objaverse_highquality_instruct_550k_0_4.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from instruct_tri2tri.tsr.system import TSR, InstructTri2Tri\n",
    "model = InstructTri2Tri.from_pretrained(\n",
    "    'instruct_tri2tri/instruct_tri2tri_config',\n",
    "    config_name=\"config.yaml\",\n",
    "    weight_name=\"model.ckpt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "image = Image.fromarray(np.zeros((512, 512, 3), dtype=np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = model([image], 'turn it to red', 'cpu', False)\n",
    "tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_codes = model([image], 'turn it to red', 'cpu', True)\n",
    "scene_codes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "data0 = json.load(open('data/objaverse/cap3d_automated_objaverse_highquality_instruct_550k_0_4.json'))\n",
    "data1 = json.load(open('data/objaverse/cap3d_automated_objaverse_highquality_instruct_550k_1_4.json'))\n",
    "data2 = json.load(open('data/objaverse/cap3d_automated_objaverse_highquality_instruct_550k_2_4.json'))\n",
    "data3 = json.load(open('data/objaverse/cap3d_automated_objaverse_highquality_instruct_550k_3_4.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "493857"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data0 + data1 + data2 + data3\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': 'objaverse/Cap3D_imgs_view0/53d0b31aa7f84bc4b1733224963d0114',\n",
       " 'caption': '3D rendering of a white sofa with wooden legs and frame.',\n",
       " 'instruct': 'make the sofa a leather sofa'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/objaverse/cap3d_automated_objaverse_highquality_instruct_550k.json', 'w') as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 41/661577 [00:15<69:07:46,  2.66it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img_name \u001b[38;5;129;01min\u001b[39;00m tqdm(img_names):\n\u001b[1;32m     12\u001b[0m     image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/objaverse/Cap3D_imgs_view0\u001b[39m\u001b[38;5;124m'\u001b[39m, img_name))\n\u001b[0;32m---> 13\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mremove_background\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrembg_session\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     image \u001b[38;5;241m=\u001b[39m resize_foreground(image, \u001b[38;5;241m0.85\u001b[39m)\n\u001b[1;32m     15\u001b[0m     image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(image)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n",
      "File \u001b[0;32m/data/wzj/instruct_tri2tri/instruct_tri2tri/tsr/utils.py:413\u001b[0m, in \u001b[0;36mremove_background\u001b[0;34m(image, rembg_session, force, **rembg_kwargs)\u001b[0m\n\u001b[1;32m    411\u001b[0m do_remove \u001b[38;5;241m=\u001b[39m do_remove \u001b[38;5;129;01mor\u001b[39;00m force\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_remove:\n\u001b[0;32m--> 413\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mrembg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremove\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrembg_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrembg_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image\n",
      "File \u001b[0;32m~/data/anaconda3/envs/pixellm/lib/python3.9/site-packages/rembg/bg.py:257\u001b[0m, in \u001b[0;36mremove\u001b[0;34m(data, alpha_matting, alpha_matting_foreground_threshold, alpha_matting_background_threshold, alpha_matting_erode_size, session, only_mask, post_process_mask, bgcolor, *args, **kwargs)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m session \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    255\u001b[0m     session \u001b[38;5;241m=\u001b[39m new_session(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mu2net\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 257\u001b[0m masks \u001b[38;5;241m=\u001b[39m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m cutouts \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mask \u001b[38;5;129;01min\u001b[39;00m masks:\n",
      "File \u001b[0;32m~/data/anaconda3/envs/pixellm/lib/python3.9/site-packages/rembg/sessions/u2net.py:29\u001b[0m, in \u001b[0;36mU2netSession.predict\u001b[0;34m(self, img, *args, **kwargs)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, img: PILImage, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[PILImage]:\n\u001b[1;32m     18\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m    Predicts the output masks for the input image using the inner session.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m        List[PILImage]: The list of output masks.\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     ort_outs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m            \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.485\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.456\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.406\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.229\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.225\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m320\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m320\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     pred \u001b[38;5;241m=\u001b[39m ort_outs[\u001b[38;5;241m0\u001b[39m][:, \u001b[38;5;241m0\u001b[39m, :, :]\n\u001b[1;32m     38\u001b[0m     ma \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(pred)\n",
      "File \u001b[0;32m~/data/anaconda3/envs/pixellm/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:220\u001b[0m, in \u001b[0;36mSession.run\u001b[0;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[1;32m    218\u001b[0m     output_names \u001b[38;5;241m=\u001b[39m [output\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs_meta]\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_feed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m C\u001b[38;5;241m.\u001b[39mEPFail \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_fallback:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import rembg\n",
    "from instruct_tri2tri.tsr.utils import remove_background, resize_foreground\n",
    "\n",
    "save_path = 'data/objaverse/images'\n",
    "img_names = os.listdir('data/objaverse/Cap3D_imgs_view0')\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "rembg_session = rembg.new_session()\n",
    "for img_name in tqdm(img_names):\n",
    "    image = Image.open(os.path.join('data/objaverse/Cap3D_imgs_view0', img_name))\n",
    "    image = remove_background(image, rembg_session)\n",
    "    image = resize_foreground(image, 0.85)\n",
    "    image = np.array(image).astype(np.float32) / 255.0\n",
    "    image = image[:, :, :3] * image[:, :, 3:4] + (1 - image[:, :, 3:4]) * 0.5\n",
    "    image = Image.fromarray((image * 255.0).astype(np.uint8))\n",
    "    image.save(f'{save_path}/{img_name}.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shangyu/Cyberpunk8T/anaconda_space/envs/langsplat/lib/python3.9/site-packages/numba/np/ufunc/parallel.py:371: NumbaWarning: \u001b[1mThe TBB threading layer requires TBB version 2021 update 6 or later i.e., TBB_INTERFACE_VERSION >= 12060. Found TBB_INTERFACE_VERSION = 12050. The TBB threading layer is disabled.\u001b[0m\n",
      "  warnings.warn(problem)\n"
     ]
    }
   ],
   "source": [
    "from instruct_tri2tri.tsr.system import InstructTri2Tri\n",
    "\n",
    "model = InstructTri2Tri.from_pretrained(\n",
    "    'instruct_tri2tri/tsr/instruct_tri2tri_config',\n",
    "    'config.yaml',\n",
    "    'model.ckpt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.instruction_converter.load_state_dict(model.backbone.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "image0 = Image.fromarray(np.zeros((512, 512, 3), dtype=np.uint8))\n",
    "image1 = Image.fromarray(np.zeros((512, 512, 3), dtype=np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 3072])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token0 = model.forward_tsr([image0], 'cpu', False)\n",
    "token0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 3072])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token1 = model([image1], 'make it green', 'cpu', False)\n",
    "token1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langsplat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
